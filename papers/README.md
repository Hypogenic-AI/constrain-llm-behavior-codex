# Downloaded Papers

1. [SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models](2303.08896_selfcheckgpt.pdf)
   - Authors: Potsawee Manakul; Adian Liusie; Mark J. F. Gales
   - Year: 2023
   - arXiv: https://arxiv.org/abs/2303.08896
   - Why relevant: Proposes a verifier-style approach to detect hallucinations and support abstention.

2. [Language Models (Mostly) Know What They Know](2207.05221_lm_know_what_they_know.pdf)
   - Authors: Saurav Kadavath; Tom Conerly; Amanda Askell; Tom Henighan; Dawn Drain; Ethan Perez; Nicholas Schiefer; Zac Hatfield-Dodds; Nova DasSarma; Eli Tran-Johnson; Scott Johnston; Sheer El-Showk; Andy Jones; Nelson Elhage; Tristan Hume; Anna Chen; Yuntao Bai; Sam Bowman; Stanislav Fort; Deep Ganguli; Danny Hernandez; Josh Jacobson; Jackson Kernion; Shauna Kravec; Liane Lovitt; Kamal Ndousse; Catherine Olsson; Sam Ringer; Dario Amodei; Tom Brown; Jack Clark; Nicholas Joseph; Ben Mann; Sam McCandlish; Chris Olah; Jared Kaplan
   - Year: 2022
   - arXiv: https://arxiv.org/abs/2207.05221
   - Why relevant: Studies self-knowledge and calibration, foundational for abstention decisions.

3. [Teaching Models to Express Their Uncertainty in Words](2205.14334_uncertainty_in_words.pdf)
   - Authors: Stephanie Lin; Jacob Hilton; Owain Evans
   - Year: 2022
   - arXiv: https://arxiv.org/abs/2205.14334
   - Why relevant: Trains models to verbalize uncertainty, directly enabling "I don't know" responses.

4. [TruthfulQA: Measuring How Models Mimic Human Falsehoods](2109.07958_truthfulqa.pdf)
   - Authors: Stephanie Lin; Jacob Hilton; Owain Evans
   - Year: 2021
   - arXiv: https://arxiv.org/abs/2109.07958
   - Why relevant: Benchmark for truthful answering and refusal behavior.

5. [Detecting Hallucinations in Large Language Model Generation: A Token Probability Approach](2405.19648_token_probability_hallucination.pdf)
   - Authors: Ernesto Quevedo; Jorge Yero; Rachel Koerner; Pablo Rivas; Tomas Cerny
   - Year: 2024
   - arXiv: https://arxiv.org/abs/2405.19648
   - Why relevant: Detection method based on token probabilities, useful as a verifier for abstention.

6. [Constitutional AI: Harmlessness from AI Feedback](2212.08073_constitutional_ai.pdf)
   - Authors: Yuntao Bai; Saurav Kadavath; Sandipan Kundu; Amanda Askell; Jackson Kernion; Andy Jones; Anna Chen; Anna Goldie; Azalia Mirhoseini; Cameron McKinnon; Carol Chen; Catherine Olsson; Christopher Olah; Danny Hernandez; Dawn Drain; Deep Ganguli; Dustin Li; Eli Tran-Johnson; Ethan Perez; Jamie Kerr; Jared Mueller; Jeffrey Ladish; Joshua Landau; Kamal Ndousse; Kamile Lukosuite; Liane Lovitt; Michael Sellitto; Nelson Elhage; Nicholas Schiefer; Noemi Mercado; Nova DasSarma; Robert Lasenby; Robin Larson; Sam Ringer; Scott Johnston; Shauna Kravec; Sheer El Showk; Stanislav Fort; Tamera Lanham; Timothy Telleen-Lawton; Tom Conerly; Tom Henighan; Tristan Hume; Samuel R. Bowman; Zac Hatfield-Dodds; Ben Mann; Dario Amodei; Nicholas Joseph; Sam McCandlish; Tom Brown; Jared Kaplan
   - Year: 2022
   - arXiv: https://arxiv.org/abs/2212.08073
   - Why relevant: Introduces policy-guided refusal and safety constraints.

7. [HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models](2305.11747_halueval.pdf)
   - Authors: Junyi Li; Xiaoxue Cheng; Wayne Xin Zhao; Jian-Yun Nie; Ji-Rong Wen
   - Year: 2023
   - arXiv: https://arxiv.org/abs/2305.11747
   - Why relevant: Benchmark focused on hallucination evaluation for LLMs.
